---
layout: page
title: Comic 004 â€“ Floating Point Lies
---

![Comic 04 â€“ Floating Point Lies](./comic.png)

*When `0.1 + 0.2` breaks your trust in math itself.* ğŸ¤¯

---

## ğŸ’¥ Problem  

Youâ€™d think **0.1 + 0.2 = 0.3**, right?  
But computers think otherwise.  
Theyâ€™ll tell you:  
> `0.1 + 0.2 = 0.30000000000000004`

Somewhere between binary math and digital logic,  
**trust issues develop.** ğŸ˜­

---

## ğŸ’» Code Example (Python)

```python
>>> 0.1 + 0.2
0.30000000000000004
````

Why? Because computers donâ€™t store numbers in decimal.
They use **binary fractions**, and some numbers like 0.1 or 0.2
canâ€™t be represented exactly in binary.

So what you get is an *approximation* â€”
a heartbreak hidden behind fifteen extra zeros.

---

## ğŸ’» Code Example (C++)

```cpp
#include <iostream>
using namespace std;

int main() {
    double a = 0.1;
    double b = 0.2;
    double c = a + b;

    cout.precision(17);
    cout << "0.1 + 0.2 = " << c << endl;

    return 0;
}
```

**Output:**

```
0.1 + 0.2 = 0.30000000000000004
```

Even C++ canâ€™t escape the floating-point truth â€”
itâ€™s not a bug, itâ€™s *binary logic.*

---

## ğŸ§  Whatâ€™s Actually Happening

Floating-point numbers follow the **IEEE 754 standard**,
which represents values as:

```
(-1)^sign Ã— 1.mantissa Ã— 2^(exponent - bias)
```

In binary, `0.1` becomes a **repeating fraction**,
just like 1/3 in decimal (0.3333...).

When your CPU tries to store it,
it rounds it to the nearest representable binary value.
Add two of these â€œalmost-accurateâ€ numbers,
and the rounding errors show up like ghosts in your output.

---

## ğŸ§© Lesson

Floating-point math isnâ€™t wrong â€” itâ€™s just **imprecise**.
So when accuracy matters (like in finance, physics, or pixel-perfect graphics):

âœ… Use `Decimal` or `Fraction` modules (Python)
âœ… Round results explicitly with `round(x, n)`
âœ… Never compare floats directly â€” compare their difference (`abs(a - b) < Îµ`)

**Rule of thumb:**
Trust integers. Doubt floats. Always verify decimals.

---

## ğŸŒ Real-World Connection

* Your favorite gameâ€™s weird physics? â†’ Float precision.
* Rounding mismatches in billing systems? â†’ Float precision.
* Machine learning gradient vanishing/exploding issues? â†’ Yep, float precision again.

Even NASAâ€™s software teams learned to *never* take decimal math for granted.

---

## ğŸ¦¸ CodeLore

Our dev sat down for a simple sum.
But when the calculator betrayed them,
they whispered to the screen:

> â€œI gave you my precision,
> and you gave me... 0.30000000000000004.â€

---

ğŸ”™ [Back to TheCodeLores Home](../../index.md)

ğŸ“… Published: November 2025
âœï¸ Author: [Aisha Karigar](https://github.com/aishakarigar)

